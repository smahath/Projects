from urllib.request import urlopen
from bs4 import BeautifulSoup
import csv
import pandas as pd

url = "https://www.census.gov/programs-surveys/popest.html"

html = urlopen(url) # Opening the URL to extract
soup = BeautifulSoup(html.read(), 'lxml'); #Creating Beautiful object and reads the html

empty_data = ""

#finds all html links and continues to find the next
for link in soup.find_all('a', href=True):
    link_end = str(link.get('href'))

#If # or / to find relative url to convert to absolute url
    if link_end[0] == "#":
        print(link_end)
        continue
    if link_end[0] == "/":
        big_url = str(url) + str(link_end)
        empty_data += big_url + "filler"
    else:
        empty_data += str(link.get('href')) + "filler"

#splits the string into a list
url_list = empty_data.split("filler")

#Writes file to each new line to csv
with open("US_Census_Bureau_links.csv",'w', newline="") as resultFile:
    cw = csv.writer(resultFile, dialect='excel')
    for url in url_list:
        cw.writerow([url])
        
file_name = "US_Census_Bureau_links.csv"
file_name_output = "US_Census_Bureau_links_without_dupes.csv"

#reads the csv file
#checks duplicates between all columns with "subset =None"
#"inplace=True provides change in data for the duplicate rows to return nothing
df = pd.read_csv(file_name)
df.drop_duplicates(subset=None, inplace=True)

#writes without dupes csv
df.to_csv(file_name_output)

#closes program
resultFile.close()
